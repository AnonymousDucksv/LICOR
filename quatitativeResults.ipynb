{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 22:24:43.092261: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-21 22:24:44.233132: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-06-21 22:24:44.233639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-21 22:24:44.289493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.289886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:06:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.815GHz coreCount: 40 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2023-06-21 22:24:44.289903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-21 22:24:44.291013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-21 22:24:44.291039: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-06-21 22:24:44.292037: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-21 22:24:44.292198: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-21 22:24:44.293195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-21 22:24:44.293671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-21 22:24:44.295651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-21 22:24:44.295755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.296185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.296528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-06-21 22:24:44.296937: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 22:24:44.297256: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-06-21 22:24:44.297324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.297683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:06:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.815GHz coreCount: 40 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2023-06-21 22:24:44.297700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-21 22:24:44.297711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-21 22:24:44.297721: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-06-21 22:24:44.297729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-21 22:24:44.297738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-21 22:24:44.297747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-21 22:24:44.297755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-21 22:24:44.297764: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-21 22:24:44.297805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.298181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.298518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-06-21 22:24:44.298542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-21 22:24:44.778070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-21 22:24:44.778098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-06-21 22:24:44.778102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-06-21 22:24:44.778293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.778680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.779037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-21 22:24:44.779369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7253 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:06:00.0, compute capability: 7.5)\n",
      "2023-06-21 22:24:44.882355: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-06-21 22:24:44.882835: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3593390000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 72, 1, 5)]        0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 72, 1, 16)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 36, 1, 16)         0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 36, 1, 32)         1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 18, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "ConvOutput (Conv2D)          (None, 18, 1, 64)         6208      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,097\n",
      "Trainable params: 8,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 22:24:45.192660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-21 22:24:45.317742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-21 22:24:45.911012: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2023-06-21 22:24:45.954146: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 6s 9ms/step - loss: 0.4318 - auc: 0.9121 - val_loss: 0.0071 - val_auc: 1.0000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0047 - auc: 1.0000 - val_loss: 0.0013 - val_auc: 1.0000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0010 - auc: 1.0000 - val_loss: 5.1912e-04 - val_auc: 1.0000\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 4.3555e-04 - auc: 1.0000 - val_loss: 2.5138e-04 - val_auc: 1.0000\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 2.2237e-04 - auc: 1.0000 - val_loss: 1.6423e-04 - val_auc: 1.0000\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1.3400e-04 - auc: 1.0000 - val_loss: 1.0716e-04 - val_auc: 1.0000\n",
      "63/63 - 0s - loss: 0.0071 - auc: 1.0000\n",
      "Test AUC score helix_1_start: 1.0\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 72, 1, 5)]        0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 72, 1, 16)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 1, 16)         0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 36, 1, 32)         1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 18, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "ConvOutput (Conv2D)          (None, 18, 1, 64)         6208      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,097\n",
      "Trainable params: 8,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4593 - auc_1: 0.9151 - val_loss: 0.0058 - val_auc_1: 1.0000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0034 - auc_1: 1.0000 - val_loss: 7.3417e-04 - val_auc_1: 1.0000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 6.5896e-04 - auc_1: 1.0000 - val_loss: 3.6269e-04 - val_auc_1: 1.0000\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 3.4759e-04 - auc_1: 1.0000 - val_loss: 1.7598e-04 - val_auc_1: 1.0000\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1.9722e-04 - auc_1: 1.0000 - val_loss: 1.1115e-04 - val_auc_1: 1.0000\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1.1954e-04 - auc_1: 1.0000 - val_loss: 8.2764e-05 - val_auc_1: 1.0000\n",
      "63/63 - 0s - loss: 0.0058 - auc_1: 1.0000\n",
      "Test AUC score helix_matching_random_amp_feat_time_no_phase: 1.0\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 72, 1, 5)]        0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 72, 1, 16)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 36, 1, 16)         0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 36, 1, 32)         1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 18, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "ConvOutput (Conv2D)          (None, 18, 1, 64)         6208      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,097\n",
      "Trainable params: 8,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.6933 - auc_2: 0.5175 - val_loss: 0.6897 - val_auc_2: 0.6463\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6847 - auc_2: 0.5905 - val_loss: 0.6630 - val_auc_2: 0.7076\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.6459 - auc_2: 0.6959 - val_loss: 0.5198 - val_auc_2: 0.9212\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4969 - auc_2: 0.8748 - val_loss: 0.4026 - val_auc_2: 0.9545\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3991 - auc_2: 0.9118 - val_loss: 0.3096 - val_auc_2: 0.9666\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.3186 - auc_2: 0.9487 - val_loss: 0.2689 - val_auc_2: 0.9738\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2421 - auc_2: 0.9738 - val_loss: 0.2245 - val_auc_2: 0.9795\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2245 - auc_2: 0.9724 - val_loss: 0.2006 - val_auc_2: 0.9836\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2087 - auc_2: 0.9743 - val_loss: 0.1821 - val_auc_2: 0.9854\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1723 - auc_2: 0.9843 - val_loss: 0.1754 - val_auc_2: 0.9878\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1634 - auc_2: 0.9837 - val_loss: 0.1441 - val_auc_2: 0.9892\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1712 - auc_2: 0.9803 - val_loss: 0.1899 - val_auc_2: 0.9899\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1657 - auc_2: 0.9813 - val_loss: 0.1600 - val_auc_2: 0.9909\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1455 - auc_2: 0.9858 - val_loss: 0.1188 - val_auc_2: 0.9920\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1100 - auc_2: 0.9922 - val_loss: 0.1501 - val_auc_2: 0.9906\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1241 - auc_2: 0.9890 - val_loss: 0.1355 - val_auc_2: 0.9921\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1111 - auc_2: 0.9914 - val_loss: 0.1176 - val_auc_2: 0.9932\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1077 - auc_2: 0.9901 - val_loss: 0.1027 - val_auc_2: 0.9936\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0962 - auc_2: 0.9918 - val_loss: 0.0878 - val_auc_2: 0.9939\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0903 - auc_2: 0.9938 - val_loss: 0.1765 - val_auc_2: 0.9953\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.1196 - auc_2: 0.9879 - val_loss: 0.0818 - val_auc_2: 0.9948\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0728 - auc_2: 0.9950 - val_loss: 0.1089 - val_auc_2: 0.9954\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0903 - auc_2: 0.9916 - val_loss: 0.0802 - val_auc_2: 0.9953\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0767 - auc_2: 0.9949 - val_loss: 0.0648 - val_auc_2: 0.9961\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0672 - auc_2: 0.9929 - val_loss: 0.0916 - val_auc_2: 0.9962\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0829 - auc_2: 0.9936 - val_loss: 0.0619 - val_auc_2: 0.9963\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0648 - auc_2: 0.9963 - val_loss: 0.0580 - val_auc_2: 0.9964\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0562 - auc_2: 0.9971 - val_loss: 0.0580 - val_auc_2: 0.9972\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0574 - auc_2: 0.9961 - val_loss: 0.0518 - val_auc_2: 0.9979\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0565 - auc_2: 0.9971 - val_loss: 0.1101 - val_auc_2: 0.9978\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0494 - auc_2: 0.9980 - val_loss: 0.0449 - val_auc_2: 0.9984\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0459 - auc_2: 0.9975 - val_loss: 0.0524 - val_auc_2: 0.9984\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0492 - auc_2: 0.9975 - val_loss: 0.0418 - val_auc_2: 0.9986\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0500 - auc_2: 0.9975 - val_loss: 0.0877 - val_auc_2: 0.9985\n",
      "63/63 - 0s - loss: 0.0518 - auc_2: 0.9979\n",
      "Test AUC score helix_matching_random_amp_feat_time: 0.9978640079498291\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 510, 1, 3)]       0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 510, 1, 16)        1456      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 255, 1, 16)        0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 255, 1, 32)        15392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 127, 1, 32)        0         \n",
      "_________________________________________________________________\n",
      "ConvOutput (Conv2D)          (None, 127, 1, 64)        61504     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 78,417\n",
      "Trainable params: 78,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 8s 183ms/step - loss: 0.6065 - auc_3: 0.7375 - val_loss: 0.3639 - val_auc_3: 0.9421\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3389 - auc_3: 0.9472 - val_loss: 0.1893 - val_auc_3: 0.9831\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1773 - auc_3: 0.9857 - val_loss: 0.1320 - val_auc_3: 0.9899\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1248 - auc_3: 0.9927 - val_loss: 0.0760 - val_auc_3: 0.9979\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1038 - auc_3: 0.9974 - val_loss: 0.0854 - val_auc_3: 0.9985\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1084 - auc_3: 0.9953 - val_loss: 0.0971 - val_auc_3: 0.9977\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.1086 - auc_3: 0.9948 - val_loss: 0.0778 - val_auc_3: 0.9936\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0934 - auc_3: 0.9958 - val_loss: 0.0472 - val_auc_3: 0.9993\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0708 - auc_3: 0.9977 - val_loss: 0.0526 - val_auc_3: 0.9993\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0483 - auc_3: 0.9989 - val_loss: 0.0347 - val_auc_3: 0.9996\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0400 - auc_3: 0.9990 - val_loss: 0.0311 - val_auc_3: 0.9997\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0483 - auc_3: 0.9968 - val_loss: 0.0160 - val_auc_3: 0.9999\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0282 - auc_3: 0.9998 - val_loss: 0.0133 - val_auc_3: 0.9999\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0317 - auc_3: 0.9994 - val_loss: 0.0111 - val_auc_3: 1.0000\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0375 - auc_3: 0.9991 - val_loss: 0.0070 - val_auc_3: 1.0000\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0246 - auc_3: 0.9989 - val_loss: 0.0105 - val_auc_3: 1.0000\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0297 - auc_3: 0.9981 - val_loss: 0.0079 - val_auc_3: 1.0000\n",
      "7/7 - 0s - loss: 0.0160 - auc_3: 0.9999\n",
      "Test AUC score blink_EEG: 0.9998989105224609\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 72, 1, 5)]        0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 70, 1, 32)         480       \n",
      "_________________________________________________________________\n",
      "tf.nn.max_pool_with_argmax ( MaxPoolWithArgmax(output= 0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense0 (Dense)               (None, 100)               3300      \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 14,031\n",
      "Trainable params: 14,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 2s 5ms/step - loss: 0.4293 - auc_4: 0.8864 - val_loss: 0.0027 - val_auc_4: 1.0000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0018 - auc_4: 1.0000 - val_loss: 4.7483e-04 - val_auc_4: 1.0000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 3.7824e-04 - auc_4: 1.0000 - val_loss: 1.7554e-04 - val_auc_4: 1.0000\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1.5647e-04 - auc_4: 1.0000 - val_loss: 8.7303e-05 - val_auc_4: 1.0000\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 7.6254e-05 - auc_4: 1.0000 - val_loss: 5.1111e-05 - val_auc_4: 1.0000\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 4.6133e-05 - auc_4: 1.0000 - val_loss: 2.9932e-05 - val_auc_4: 1.0000\n",
      "63/63 - 0s - loss: 0.0027 - auc_4: 1.0000\n",
      "Test AUC score helix_1_start: 1.0\n",
      "WARNING:tensorflow:5 out of the last 401 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7c1c3ac5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 72, 1, 5)]        0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 70, 1, 32)         480       \n",
      "_________________________________________________________________\n",
      "tf.nn.max_pool_with_argmax_1 MaxPoolWithArgmax(output= 0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense0 (Dense)               (None, 100)               3300      \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 14,031\n",
      "Trainable params: 14,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.2958 - auc_5: 0.9393 - val_loss: 8.0269e-04 - val_auc_5: 1.0000\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 3.8851e-04 - auc_5: 1.0000 - val_loss: 8.4434e-05 - val_auc_5: 1.0000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 7.3054e-05 - auc_5: 1.0000 - val_loss: 5.9586e-05 - val_auc_5: 1.0000\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 4.0793e-05 - auc_5: 1.0000 - val_loss: 2.1091e-05 - val_auc_5: 1.0000\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 2.3768e-05 - auc_5: 1.0000 - val_loss: 1.2329e-05 - val_auc_5: 1.0000\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1.3274e-05 - auc_5: 1.0000 - val_loss: 8.3339e-06 - val_auc_5: 1.0000\n",
      "63/63 - 0s - loss: 8.0269e-04 - auc_5: 1.0000\n",
      "Test AUC score helix_matching_random_amp_feat_time_no_phase: 1.0\n",
      "WARNING:tensorflow:5 out of the last 6654 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7c1f51f3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 72, 1, 5)]        0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 70, 1, 32)         480       \n",
      "_________________________________________________________________\n",
      "tf.nn.max_pool_with_argmax_2 MaxPoolWithArgmax(output= 0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense0 (Dense)               (None, 100)               3300      \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 14,031\n",
      "Trainable params: 14,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5847 - auc_6: 0.7531 - val_loss: 0.0432 - val_auc_6: 0.9999\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0453 - auc_6: 0.9984 - val_loss: 0.0218 - val_auc_6: 1.0000\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0131 - auc_6: 0.9999 - val_loss: 0.0335 - val_auc_6: 1.0000\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0135 - auc_6: 0.9997 - val_loss: 0.0112 - val_auc_6: 1.0000\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0090 - auc_6: 1.0000 - val_loss: 0.0028 - val_auc_6: 1.0000\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0120 - auc_6: 0.9999 - val_loss: 0.0048 - val_auc_6: 1.0000\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0245 - auc_6: 0.9981 - val_loss: 0.0319 - val_auc_6: 1.0000\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.0075 - auc_6: 0.9995 - val_loss: 0.0033 - val_auc_6: 1.0000\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 8.8287e-04 - auc_6: 1.0000 - val_loss: 0.0058 - val_auc_6: 1.0000\n",
      "63/63 - 0s - loss: 0.0112 - auc_6: 1.0000\n",
      "Test AUC score helix_matching_random_amp_feat_time: 0.9999849796295166\n",
      "WARNING:tensorflow:5 out of the last 6704 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7c1f51f1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 510, 1, 3)]       0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 481, 1, 32)        2880      \n",
      "_________________________________________________________________\n",
      "tf.nn.max_pool_with_argmax_3 MaxPoolWithArgmax(output= 0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Dense0 (Dense)               (None, 200)               6600      \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 29,931\n",
      "Trainable params: 29,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 51ms/step - loss: 0.6751 - auc_7: 0.6893 - val_loss: 0.4894 - val_auc_7: 0.9996\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3341 - auc_7: 0.9989 - val_loss: 0.0261 - val_auc_7: 1.0000\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0177 - auc_7: 0.9999 - val_loss: 0.0564 - val_auc_7: 1.0000\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0277 - auc_7: 0.9995 - val_loss: 0.0394 - val_auc_7: 1.0000\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0266 - auc_7: 0.9995 - val_loss: 0.0412 - val_auc_7: 1.0000\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0886 - auc_7: 0.9913 - val_loss: 0.0142 - val_auc_7: 1.0000\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0356 - auc_7: 0.9995 - val_loss: 0.0025 - val_auc_7: 1.0000\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0052 - auc_7: 1.0000 - val_loss: 0.0100 - val_auc_7: 1.0000\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0160 - auc_7: 0.9992 - val_loss: 0.0020 - val_auc_7: 1.0000\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0082 - auc_7: 1.0000 - val_loss: 7.3440e-04 - val_auc_7: 1.0000\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0070 - auc_7: 1.0000 - val_loss: 0.0021 - val_auc_7: 1.0000\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.0203 - auc_7: 0.9998 - val_loss: 0.0015 - val_auc_7: 1.0000\n",
      "7/7 - 0s - loss: 0.0025 - auc_7: 1.0000\n",
      "Test AUC score blink_EEG: 1.0\n",
      "WARNING:tensorflow:5 out of the last 6604 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7c1c345f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Test AUC score helix_1_start: 1.0\n",
      "Test AUC score helix_matching_random_amp_feat_time_no_phase: 1.0\n",
      "Test AUC score helix_matching_random_amp_feat_time: 0.8049999999999999\n",
      "Test AUC score blink_EEG: 1.0\n"
     ]
    }
   ],
   "source": [
    "from data_constructor import Dataset\n",
    "from model_constructor import Model\n",
    "from interpreter_constructor import Interpreter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_fraction = 0.8\n",
    "seed_int = 54321 # 54321\n",
    "\n",
    "subset = 'test'\n",
    "\n",
    "DATASET_LABELS = {'helix_1_start':\"Sine\",'helix_matching_random_amp_feat_time_no_phase':\"SineRandom\",'helix_matching_random_amp_feat_time':\"SineRandomPhase\",\"blink_EEG\":\"Blink EEG\"}\n",
    "MODELS_LABELS  = {'relu-mlp':'MLP LLM','conv-relu-mlp':'LICOR','CAM-conv':'CNN CAM'} \n",
    "\n",
    "AUC_METRICS = []\n",
    "N_splits = 5\n",
    "datasets = ['helix_1_start','helix_matching_random_amp_feat_time_no_phase','helix_matching_random_amp_feat_time','blink_EEG'] #'circles','moons','helix_1_start','helix_2_start','helix_1_random_time','helix_2_random_time','helix_random_feat_random_time','helix_matching_random_amp_feat_time', 'helix_matching_random_amp_feat_time_no_phase'\n",
    "models = ['CAM-conv','conv-relu-mlp','relu-mlp'] # 'relu-mlp', 'conv-relu-mlp', 'CAM-conv'\n",
    "SPARSITY = []\n",
    "for i, model_str in enumerate(models):\n",
    "    for ii, data_str in enumerate(datasets):\n",
    "        data = Dataset(data_str, train_fraction, seed_int)\n",
    "        model = Model(seed_int, data, model_str)\n",
    "        _,test_auc = model.train_model()\n",
    "        idx_list = range(199) \n",
    "        interpreter = Interpreter(data, model, idx_list, subset='test',use_bias=False)\n",
    "        SPARSITY.append([model_str,data_str,interpreter.getSparsity()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{4}{l}{Sparsity} \\\\\n",
      "Dataset & blink\\_EEG & helix\\_1\\_start & helix\\_matching\\_random\\_amp\\_feat\\_time & helix\\_matching\\_random\\_amp\\_feat\\_time\\_no\\_phase \\\\\n",
      "Model         &           &               &                                     &                                              \\\\\n",
      "\\midrule\n",
      "CAM-conv      &     0.945 &         0.999 &                               0.997 &                                        0.999 \\\\\n",
      "conv-relu-mlp &     0.041 &         0.282 &                               0.299 &                                        0.210 \\\\\n",
      "relu-mlp      &     0.163 &         0.545 &                               0.706 &                                        0.864 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16574/420225919.py:4: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df_sparsity.to_latex(float_format='%1.3f'))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_sparsity = pd.DataFrame(SPARSITY, columns = [\"Model\", \"Dataset\",\"Sparsity\"])\n",
    "df_sparsity = df_sparsity.pivot(index='Model',columns='Dataset')\n",
    "df_sparsity \n",
    "print(df_sparsity.to_latex(float_format='%1.3f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsCF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
